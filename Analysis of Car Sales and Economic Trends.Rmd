---
title: "Analysis of Car Sales and Economic Trends"
author: "Tommy Tight"
date: "2024-05-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = FALSE)
library(forecast)
library(tidyverse)
library(data.table)
library(car)
```

#### QUESTION 2 - For the variable "Gold" (column R in the data file), subset the data to include the 2nd and 3rd quartile. Select the most appropriate chart/graph/plot type and create a well-formatted plot of your subset data.
```{r, error=FALSE}
#read in and subset data
premium_data <- read_csv("~/Library/CloudStorage/GoogleDrive-ttight@nd.edu/My Drive/Past Semesters/Spring Semester - Sophomore Year /Predictive Analytics/Final Exam Prep/A0 Premium.csv")
premium_data <- premium_data[1:162, ]

#Check for missing values 
summary(premium_data)

#The variables gasoline through aluminium each have one missing value, so I 
#will remove the missing values from the dataset 
premium_data <- na.omit(premium_data)


#Subsetting data to include the second and third quartile of Gold variable

q_2 <- quantile(premium_data$Gold, 0.25)
q_3 <- quantile(premium_data$Gold, 0.75)


gold_q2_q3 <- premium_data$Gold[premium_data$Gold >= q_2 & premium_data$Gold <= q_3]

hist(gold_q2_q3, col = "navy", main = "Q2 and Q3 of Gold Prices", xlab = "Price of Gold", xlim = c(800, 1400))

```


#### QUESTION 3 - 1.) Examine your car sales variable in column B Create a well-formatted plot of your data. What could be possible time-series components present in your data? (Do not detrend - this is not necessary. I am asking you to evaluate the presence of time-series components based on what you know and see.) 2.) Examine your car sales variable against all variables in the dataset. Provide three variables with strongest relationship with your car sales. 3.) Write a brief explanation how these three variables could affect your car sales variable (no calculations are necessary).
```{r}
#Make data time-series. Only goes to May of 2018 due to removal of row 162.
premium_ts <- ts(premium_data$`A0 Premium`, frequency = 12, 
                 start = c(2005, 1), 
                 end = c(2018, 5))

#Plot time-series premium data
plot(premium_ts,
     main = "Premium Car Sales over time", 
     col = "navy",
     xlab = "Year", 
     ylab = "Car Sales")

#Check correlation of car sales against all other variables in the dataset. 
#Removing column 1, as the date will not have correlation with car sales. 
premium_matrix <- as.matrix(premium_data[ , -1])
cor(premium_matrix)
```


1.) One time-series trend that I think could be a part of the data is a cycle that starts around 2009. Here, the data seems to change from being linear to being quadratic, before returning to what appears to be a more linear trend again, which is indicative of a cycle. Additionally, there appears to be a linear trend, as the data overall seems to increase linearly. My guess is that after I remove the cycle, the linear trend will become much more visible. There are likely other trends, including power trends and seasonality in the data, but they are not very visible at this point. There is also always random noise and signal in time-series data. 



2.) Car sales (A0.Premium) has the strongest correlation with: Gold (0.78111455), Silver (0.75588166), and Imports (0.74831460) in that order. 



3.) All three of these variables have strong, positive linear relationships with A0.Premium. This means that, as these variables increase, car sales for this type of car will also tend to increase. Increases in the value of gold and silver, as well as increasing imports could be indicative of poor economic performace, which could lead to only wealthy people, who are buying premium cars, being able to buy cars. This could mean increases in premium car sales. 





## QUESTION 4 

* Your dependent variable is Industrial productivity.
* Your independent variable is Exports.

1.) Use the entire length of the dataset to examine your dependent variable. Make a decision on the length of your training and validation dataset. Choose whether to partition now or after detrending (if necessary).

2.) Fully detrend your DV if necessary. What is the variance explained by the time-related systematic noise?

3.) Decide whether the ARIMA-family model is appropriate for your dependent variable. Explain your reasoning.

4.) Examine the relationship between your dependent and independent variable. Create up to six lags and find the strongest (lagged) variable to use in your model.

5.) Create an appropriate model on the training data based on your answer to Q3 of this section. Make sure you follow all the necessary steps to build an appropriate model.

6.) Create a forecast on your validation dataset. 

7.) Graph your results, showing the actual data, model results on training data, forecast on validation data.

8.) Write a one-paragraph explanation of your modeling and comment on your results.

#### Parts 1 and 2
```{r}
#Extracting Industrial Productivity from the dataset and making it time-series. 
ind_prod <- premium_data$`Industrial productivity`
ind_prod_ts <- ts(ind_prod, frequency = 12, 
                  start = c(2005, 1), 
                 end = c(2018, 5))
plot(ind_prod_ts, type = "l", col = "navy")

#determining training and validation data

#161 total observations, validation data should be 20 - 40%.
#161 * 0.25 = 40.25. I want my validation data to be a whole number of years, 
# so I am going to make nvalid = 36, which is 3 years of data and about 
#20% of the total data. 

# There is a large dip in the Industrial Productivity that starts in about 2008,
#likely due to the stock market crash and recession. However, outside of this 
#dip, the validation data does not seem to be too different from the training 
#data. Additionally, I might be able to remove the affects of this dip when I 
#detrend for a structural break. Therefore, I am going to detrend the validation 
#data and training data together. 


#Detrending dependent variable Industrial Productivity


#Detrending for linear trend
ind_prod_dt_1 <- tslm(ind_prod_ts ~ trend)
summary(ind_prod_dt_1)
plot(ind_prod_dt_1$residuals)

#linear trend accounts for 77.5% of variability in the Industrial Productivity

#Trying to account for dip, by detrending for structural break that occurs 
#around the year 2008

#Creating a counter for all of the rows in the dataset
t <- seq(1:161)

#The structural break occurs 3 years, or 36 months into the dataset. 
ind_prod_SB <- ifelse(t < 36, 0, 1)

ind_prod_dt_2 <- tslm(ind_prod_ts ~ trend + ind_prod_SB)
summary(ind_prod_dt_2)
plot(ind_prod_dt_2$residuals)

#Linear trend and Structural break account for 79.75% of the variability in the 
#Industrial Productivity variable 

#detrending for power trends
ind_prod_dt_3 <- tslm(ind_prod_ts ~ trend + 
                        ind_prod_SB + 
                        I(trend^2) + I(trend^3) + I(trend^4))
summary(ind_prod_dt_3)
plot(ind_prod_dt_3$residuals)

#Extracting signal
ind_prod_signal <- ind_prod_dt_3$residuals

#Renaming variable
ind_prod_detrend <- ind_prod_dt_3

```



I am done detrending. The time-related systematic noise explains about 81.58% of the variance in industrial productivity.


#### Parts 3 and 4 
```{r}
#Running a durbin-watson test to check if there is auto-correlation in the
#Industrial Productivity variable.
durbinWatsonTest(ind_prod_detrend)

#Creating a dataset with only my dependent and independent variables 

ind_prod_data <- premium_data[ , c(20, 10)]

#Creating lags for the independent variable Exports 
setDT(ind_prod_data)[ , paste0("export_lag", 1:6):= shift(Exports, 1:6)]

#Testing the relationship between the Iv, lags, and the DV
matrix_2 <- as.matrix(ind_prod_data)
cor(matrix_2, use = "pairwise.complete.obs")

```


* The p-value of the durbin-watson test is below 0.05, so it is significant, additionally, the autocorrelation value is very high (0.8792) so it is safe to assume there is auto-correlation in the data. Additionally, because I want to consider the effect of an independent variable (Exports) on the dependent variable (Industrial Productivity), I will be using an ARMAX model. 

* Industrial Production has the strongest correlation with Exports, not any of the lags for exports, so I will use exports as my independent variable in my ARMAX


#### Parts 5 - 7
```{r}
######Detrending independet variable to use in ARMAX###### 

export_ts <- ts(ind_prod_data$Exports, 
               frequency = 12, 
               start = c(2005, 1), 
               end = c(2018, 5))

plot(export_ts)

#Appears to be a cycle in the data where it dips every few years and then 
#increases. 

#Detrending for cycle 
t <- seq(1:161)
#Picked a period of 60 months as it looks like the cycle occurs around every 5 years
cyc1 <- sin(2*pi*t/120)
cyc2 <- cos(2*pi*t/120)

export_dt_1 <- tslm(export_ts ~ cyc1 + cyc2)
summary(export_dt_1)
plot(export_dt_1$residuals)

#Linear detrend 
export_dt_2 <- tslm(export_ts ~ cyc1 + cyc2 + 
                      trend)
summary(export_dt_2)
plot(export_dt_2$residuals)

#Power detrend 
export_dt_3 <- tslm(export_ts ~ cyc1 + cyc2 + 
                      trend+ 
                      I(trend^2) + I(trend^3))
summary(export_dt_3)
plot(export_dt_3$residuals)

exports_detrend <- export_dt_3


#extracting signal 
exports_signal <- export_dt_3$residuals

#I am done detrending the independent variable exports. The  time-related 
#systematic noise accounted for 73.23% of the variance in the exports variable.

########Creating ARMAX Model###########


#First, make a new data table with detrended variables
ind_prod_armax_data <- data.frame(ind_prod_signal, 
                               exports_signal)



#because I made the lags earlier on, I effectively lost the first 6 rows of data
#since they now have N/A values. Therefore, I will need to drop these rows 
#before running the ARMAX
ind_prod_sig_drop <- ts(ind_prod_armax_data$ind_prod_signal[7:161])
export_lag_drop <- ts(ind_prod_armax_data$exports_signal[7:161])
ind_prod_armax_data <- data.frame(ind_prod_sig_drop, export_lag_drop)



#Checking to see what AR and MA components exist 
par(mfrow = c(1, 2))
ind_prod_ACF_1 <- Acf(ind_prod_signal)
ind_prod_PACF_1 <- Pacf(ind_prod_signal)

#AR: 1
#MA: None


#Creating the ARMAX model
#We use 1:118 since we only want to include training data. Our training data 
#used to be 1:124, but since we took out 6 rows it is now 1:118
ind_prod_ARMAX_1 <- Arima(ts(ind_prod_armax_data$ind_prod_sig_drop[1:118]), 
                       order = c(1,0,0), 
                       xreg = ind_prod_armax_data[1:118, c(-1)])

#Checking to see if we have any remaining AR and MA components
par(mfrow = c(1,2))
ind_prod_ACF_2 <- Acf(ind_prod_ARMAX_1$residuals, 
                   col = "pink", 
                   main = "Ind_Prod ACF")
ind_prod_PACF_2 <- Pacf(ind_prod_ARMAX_1$residuals, 
                     col = "limegreen", 
                     main = "Ind_Prod PACF" )

#AR:10
#MA:10

#Rerunning ARMAX model 
ind_prod_ARMAX_2 <- Arima(ts(ind_prod_armax_data$ind_prod_sig_drop[1:118]), 
                       order = c(10,0,0), 
                       xreg = ind_prod_armax_data[1:118, c(-1)])

#Checking to see if we have any remaining AR and MA components
par(mfrow = c(1,2))
ind_prod_ACF_3 <- Acf(ind_prod_ARMAX_2$residuals, 
                   col = "pink", 
                   main = "Ind_Prod ACF")
ind_prod_PACF_3 <- Pacf(ind_prod_ARMAX_2$residuals, 
                     col = "limegreen", 
                     main = "Ind_Prod PACF" )

#No remaining AR or MA components, ARMAX model is complete. 



#Creating a forecast for validation dataset 

#we use the remaining 36 obersvations as our validation data 
nvalid_ARMAX <- 36

#We are using the ind_prod_ARMAX_2 model to try to predict the future values of our 
#validation data, which is the remaining 36 data points.  
ARMAX_forecast <- predict(ind_prod_ARMAX_2, 
                    newxreg = as.matrix(ind_prod_armax_data$export_lag_drop[119:155]), 
                    h = 36) 

#Making the training set
ARMAX_training_noise <- ind_prod_detrend$fitted.values[1:118]
ARMAX_training_signal <- ind_prod_ARMAX_2$fitted[1:118]
ARMAX_training_total <- ARMAX_training_signal + ARMAX_training_noise

#making the validation set
ARMAX_valid_noise <- ind_prod_detrend$fitted.values[119:155]
ARMAX_valid_signal <- ARMAX_forecast$pred
ARMAX_valid_total <- ARMAX_valid_noise + ARMAX_valid_signal


#Before we can plot the forecast, we need to change the variables so that they
#time-series and all in terms of years and months. 

#Training data starts 6 months later since we removed first 6 rows due to lags
ARMAX_training_total <- ts(ARMAX_training_total, 
                           frequency = 12, 
                           start = c(2005, 7), 
                           end = c(2015, 6))

#Validation data starts right after training
ARMAX_valid_total <- ts(ARMAX_valid_total, 
                        frequency = 12, 
                        start = c(2015, 7), 
                        end = c(2018, 6))

#Visualizing the forecast
par(mfrow = c(1,1))
plot(ind_prod_ts, 
     ylab = "Industrial Productivity",
     xlab = "Year", 
     xlim = c(2005, 2019),
     ylim = c(80, 130), 
     type = "l", 
     main = "Industrial Productivity ARMAX Forecast", 
     lty = 2)
lines(ARMAX_training_total, lwd = 2, col = "plum")
lines(ARMAX_valid_total, lwd = 2, col = "navy")
lines(c(2015.5, 2015.5), c(-1000, 120000))
legend("topleft", 
       inset = c(0, 0), 
       legend = c("ARMAX on Training Data", "ARMAX on Validation Data"), 
       col = c("plum", "navy"), 
       pch = 1, 
       cex = 0.5)



```


**8.)Write a one-paragraph explanation of your modeling and comment on your results.** 


I used and ARMAX model in order to forecast the dependent variable of Industrial Productivity. I used and ARMAX model because I wanted to consider the effect the independent variable Exports has on Industrial Productivity, as well as the effect of the auto-regressive and moving averages components of the Industrial Productivity variable. In order to create this model, I first had to detrend  my dependent variable. Then, I ran a durbin-watson test to check for auto-correlation. I determined there was auto-correlation, so I went forward with the ARMAX model. I then went on to detrend my independent variable for time-related systematic noise. After doing so, I created lags of the independent variable to see if one of the lags had a greater correlation with the dependent variable. As it turns out, the independent variable itself had the greatest correlation with the dependent variable, so that is the one I used in my model. I then had a to create a new dataset with the signal I extracted from my detrending, making sure to account for the 6 observations I lost to creating lags earlier in the process. After that, I checked for AR and MA components using the ACF and PACF functions. I added the AR components to the ARMAX model that I built for the training data and checked the AR and MA components again. I repeated this process until there were no remaining AR or MA components. Finally, I forecasted my validation data using my ARMAX model that was trained on the training data and graphed the results. Overall, I am very happy with the results of the model. It was difficult to make this forecast due to the structural break that occurred in 2008, and this is why there is a large drop in the forecast of the training data when there isn't a large drop in the actual Industrial Productivity data. However, this model does a very good job of predicting the validation data and the graph of forecast for the validation data matches the actual data very accurately. I do not have another model to compare my model to, so it doesn't make sense to look at the summary statistics since those are mostly relative. But, just based on the graph of the forecast, this model is very good. 



## Question 5

* Your DV is Finished Goods
* Your IVs are USD, BCI, Unemp, Aluminum, and Industrial Productivity.
For your DV, build a logistic regression model. 

1. Create a training and a validation dataset.

2. Build a logistic regression model on the training dataset. Which variables are significant?

3. Calculate the probability of FG sufficiency using the mean values of each of the independent variables.

4. Build a prediction on a validation data using logistic regression.

5. Plot a lift chart. Explain your results.

```{r}
#setting seed
set.seed(2)

#creating a new dataset with only the variables I will need 
log_reg_data <- premium_data[, c(5, 7, 11, 19, 20, 22)]


#Creating training and validation data sets
#0.75 indicates that 75 percent of the data will be training.
train.index <- sample(c(1:dim(log_reg_data)[1]), dim(log_reg_data)[1]*0.75)  
train.df <- log_reg_data[train.index, ]
valid.df <- log_reg_data[-train.index, ]


#creating the logistic regression model
log_reg_model <- glm(train.df$`Finished Goods (1=efficient, 0 - not)`~., 
                        data = train.df, 
                        family = "binomial") 
options(scipen=999)
summary(log_reg_model)

#USD and Unemp are significant predictors of Finished Goods Inventory at a 
#significance level of 0.05. The price of an ounce of aluminium is
#significant at a significance level of 0.1, and it is very close to 0.05.
##Additionally, the residual deviance is much smaller than the null deviance, 
#indicating that our model is effective.

#Finding mean values of independent variables 
summary(log_reg_data)

#USD: 37.59
#BCI: 100.07
#Unemp: 0.0626
#Aluminium: 2073
#Industrial Productivity: 105.93

#Finding probability of FG Sufficiency

#creating logit equation. The 1 represents that we want to predict sufficiency,
#so we multiply the intercept coefficient by 1. All of other numbers are the
#mean value of the predictors.
logit <- c(1, 37.59, 100.07, 0.0626, 2073, 105.93) %*% log_reg_model$coefficients
logit #0.1199637

log_odds <- exp(logit)
log_odds #1.127456 

prob <- log_odds/(1+log_odds)
prob #0.529955

#There is a 53% chance that this set of coefficients for the independent 
#variables leads to us classifying Finished Goods as sufficient. Because this
#value is above, 50%, we would classify as sufficient. 

#Testing to see how the regression does with the validation data.
log_reg_pred <- predict(log_reg_model, valid.df[ , -6], type = "response")

#getting the first 5 actual and predicted values. 
data.frame(actual = valid.df$`Finished Goods (1=efficient, 0 - not)`[1:5], 
           predicted = log_reg_pred[1:5])

#The model correctly classified 4 of the first 5 values of the validation data. 


#Graph results 
suppressPackageStartupMessages(library(gains))
gain <- gains(valid.df$`Finished Goods (1=efficient, 0 - not)`, log_reg_pred, groups = 10)

#plot lift chart
par(mfrow = c(1, 2))
plot(c(0,gain$cume.pct.of.total*sum(valid.df$`Finished Goods (1=efficient, 0 - not)`))~c(0,gain$cume.obs),
xlab="# cases", ylab="Cumulative", main="Plot Lift Chart",
type="l", col="darkgreen")
lines(c(0,sum(valid.df$`Finished Goods (1=efficient, 0 - not)`))~c(0, dim(valid.df)[1]), lty=2)

#plot decile-wise chart 
heights <- gain$mean.resp/mean(valid.df$`Finished Goods (1=efficient, 0 - not)`)
midpoints <- barplot(heights, names.arg = gain$depth, ylim = c(0, 2.5),
xlab = "Percentile", ylab = "Mean Response",
main = "Decile-wise lift chart", col="lightblue")
text(midpoints, heights +0.075, labels=round(heights, 1), cex = 0.8)



```


**Interpretation of Lift Chart**


The model does a fairly good job of correctly classifying whether Finished Goods will be sufficient or not. The lift curve is fairly far away from the baseline, indicating that the model does a much better job of classifying the sufficiency of finished goods than simply guessing. Additionally, the model is 1.9 times better at predicting the classification of sufficiency of finished goods for the top 10% of the cases based on the independent variables I used. 



















